{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/l6wu/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#tensorflow 2.2.2\n",
    "#others newest\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tqdm.keras as tk\n",
    "# from keras_tqdm import TQDMNotebookCallback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./new_train/new_train\"\n",
    "test_path = \"./new_val_in/new_val_in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205942\n",
      "205942\n",
      "185300\n",
      "20500\n"
     ]
    }
   ],
   "source": [
    "data_list = glob(os.path.join(train_path, '*'))\n",
    "random.shuffle(data_list)\n",
    "print(int(len(data_list)*1))\n",
    "data_list = data_list[:len(data_list)]\n",
    "print(int(len(data_list)*1))\n",
    "\n",
    "train_list = data_list[:int(len(data_list)*0.9)-47]\n",
    "valid_list = data_list[int(len(data_list)*0.9)+95:]\n",
    "print(len(train_list))\n",
    "print(len(valid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "latent_dim = 256\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_y(train_list, batch_size):\n",
    "    index = 0\n",
    "    # shape of return data (batch_size, 19, [position + velocity])\n",
    "    # shape of return data (batch_size, 30, [position + velocity])\n",
    "    while True:\n",
    "        encoder_input_data = np.zeros((batch_size, 19, 2), dtype='float32')\n",
    "        decoder_input_data = np.zeros((batch_size, 30, 2), dtype='float32')\n",
    "        decoder_target_data = np.zeros((batch_size, 30, 2), dtype='float32')\n",
    "        for i in range(batch_size):\n",
    "            train_path = train_list[index]\n",
    "            with open(train_path, 'rb') as f:\n",
    "                data = pickle.load(f)                \n",
    "                agent_id = data['agent_id']\n",
    "                idx = np.where(data[\"track_id\"] == data[\"agent_id\"])[0][0]\n",
    "                \n",
    "                v_in_x = data[\"v_in\"][idx,:,1]\n",
    "                p_in_x = data[\"p_in\"][idx,:,1]\n",
    "                v_out_x = data[\"v_out\"][idx,:,1]\n",
    "                p_out_x = data[\"p_out\"][idx,:,1]\n",
    "                \n",
    "                encoder_input_data[i,:,0] = p_in_x\n",
    "                encoder_input_data[i,:,1] = v_in_x\n",
    "                \n",
    "                decoder_input_data[i,0,0] = p_in_x[-1]\n",
    "                decoder_input_data[i,0,1] = v_in_x[-1]\n",
    "                decoder_input_data[i,1:,0] = p_out_x[:-1]\n",
    "                decoder_input_data[i,1:,1] = v_out_x[:-1]\n",
    "                \n",
    "                decoder_target_data[i,:,0] = p_out_x\n",
    "                decoder_target_data[i,:,1] = v_out_x\n",
    "               \n",
    "            index += 1\n",
    "        if index == len(train_list):\n",
    "            index = 0\n",
    "\n",
    "        yield [encoder_input_data, decoder_input_data], decoder_target_data\n",
    "\n",
    "def valid_generator_y(valid_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        encoder_input_data = np.zeros((batch_size, 19, 2), dtype='float32')\n",
    "        decoder_input_data = np.zeros((batch_size, 30, 2), dtype='float32')\n",
    "        decoder_target_data = np.zeros((batch_size, 30, 2), dtype='float32')\n",
    "        for i in range(batch_size):\n",
    "            valid_path = valid_list[index]\n",
    "            with open(valid_path, 'rb') as f:\n",
    "                data = pickle.load(f)                \n",
    "                agent_id = data['agent_id']\n",
    "                idx = np.where(data[\"track_id\"] == data[\"agent_id\"])[0][0]\n",
    "                \n",
    "                v_in_y = data[\"v_in\"][idx,:,1]\n",
    "                p_in_y = data[\"p_in\"][idx,:,1]\n",
    "                v_out_y = data[\"v_out\"][idx,:,1]\n",
    "                p_out_y = data[\"p_out\"][idx,:,1]\n",
    "                \n",
    "                encoder_input_data[i,:,0] = p_in_y\n",
    "                encoder_input_data[i,:,1] = v_in_y\n",
    "                \n",
    "                decoder_input_data[i,0,0] = p_in_y[-1]\n",
    "                decoder_input_data[i,0,1] = v_in_y[-1]\n",
    "                decoder_input_data[i,1:,0] = p_out_y[:-1]\n",
    "                decoder_input_data[i,1:,1] = v_out_y[:-1]\n",
    "                \n",
    "                decoder_target_data[i,:,0] = p_out_y\n",
    "                decoder_target_data[i,:,1] = v_out_y\n",
    "                \n",
    "            index += 1\n",
    "        if index == len(valid_list):\n",
    "            index = 0\n",
    "\n",
    "        yield [encoder_input_data, decoder_input_data], decoder_target_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#encoder portion\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True, activation=\"relu\")\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder portion\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, activation=\"relu\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_y = train_generator_y(train_list,100)\n",
    "valid_gen_y = valid_generator_y(valid_list,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "checkpointer = ModelCheckpoint(filepath='./lstm_seqtoseq_y_weights.hdf5', verbose=2, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/l6wu/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 171.98742, saving model to ./lstm_seqtoseq_y_weights.hdf5\n",
      "1853/1853 - 426s - loss: 338.5666 - val_loss: 171.9874\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 171.98742 to 103.02857, saving model to ./lstm_seqtoseq_y_weights.hdf5\n",
      "1853/1853 - 424s - loss: 146.1898 - val_loss: 103.0286\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 103.02857 to 68.06461, saving model to ./lstm_seqtoseq_y_weights.hdf5\n",
      "1853/1853 - 428s - loss: 84.6343 - val_loss: 68.0646\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 68.06461 to 41.17521, saving model to ./lstm_seqtoseq_y_weights.hdf5\n",
      "1853/1853 - 554s - loss: 54.8671 - val_loss: 41.1752\n",
      "Epoch 5/10\n"
     ]
    }
   ],
   "source": [
    "hist_y = model.fit(train_gen_y,\n",
    "                        verbose=2,\n",
    "                        epochs=10,\n",
    "                        validation_data=valid_gen_y,\n",
    "                        steps_per_epoch=(len(train_list)/100),\n",
    "                        validation_steps=(len(valid_list)/100),\n",
    "                        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainERR=hist_y.history['loss']\n",
    "ValidERR=hist_y.history['val_loss']\n",
    "# print('@%f, Minimun error:%f, at iteration: %i' % (hist.history['val_loss'][epoch-1], np.min(np.asarray(ValidERR)),np.argmin(np.asarray(ValidERR))+1))\n",
    "print('drawing the training process...')\n",
    "plt.figure(2)\n",
    "plt.plot(range(1,10+1),TrainERR,'b',label='TrainERR')\n",
    "plt.plot(range(1,10+1),ValidERR,'r',label='ValidERR')\n",
    "plt.xlim([1,epochs])\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.grid(True)\n",
    "fig1 = plt.gcf()\n",
    "fig1.savefig('lstm_p+v_x.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder model inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "#decoder model inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    #input seq is a scene \n",
    "    # Encode the input as state vectors.\n",
    "#     print(\"input seq shape is \",input_seq.shape)\n",
    "    input_shaped = np.empty((1,19,2))\n",
    "    input_shaped[0,:,:] = input_seq\n",
    "#     encoder_model.summary()\n",
    "#     input_shaped = np.reshape(input_seq, (input_seq.shape[0], 1, input_seq.shape[1]))\n",
    "#     print(\"inpiut shaped is \", input_shaped)\n",
    "    states_value = encoder_model.predict(input_shaped)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0,0,:] = input_seq[len(input_seq)-1] \n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    index = 0\n",
    "    decoded_sentence = ''\n",
    "    decoded_array = np.empty((30,2))\n",
    "    while index < 30:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        \n",
    "#         print(\"output tokens are \", output_tokens[0][0])\n",
    "        \n",
    "        # Sample a token\n",
    "        decoded_array[index,:] = output_tokens[0][0]\n",
    "\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, :] = output_tokens\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "        index += 1\n",
    "    \n",
    "    return decoded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "columns.append(\"ID\")\n",
    "for i in range(30):\n",
    "    num = \"v\"+str(i+1)\n",
    "    columns.append(num)\n",
    "    \n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing portion\n",
    "test_list = glob(os.path.join(test_path, '*'))\n",
    "print(len(test_list))\n",
    "\n",
    "for x in tqdm(test_list):\n",
    "    test_x = np.empty((30,2))\n",
    "    with open(x, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        new_row = []\n",
    "        scene_id = data['scene_idx']\n",
    "        new_row.append(scene_id)\n",
    "        agent_id = data['agent_id']\n",
    "        idx = np.where(data[\"track_id\"] == data[\"agent_id\"])[0][0]   \n",
    "        input_data = np.empty((19,2))\n",
    "        input_data[:,0] = data['p_in'][idx,:,1]\n",
    "        input_data[:,1] = data['v_in'][idx,:,1]\n",
    "        \n",
    "#         input_data = input_data[..., np.newaxis]\n",
    "        test_x = decode_sequence(input_data)\n",
    "        flat = test_x.flatten()\n",
    "        for i in range(30):\n",
    "            new_row.append(flat[i*2])\n",
    "#         for elem in flat:\n",
    "#             new_row.append(elem)\n",
    "        df_length = len(df)\n",
    "        df.loc[df_length] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df['ID'].map(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_test = []\n",
    "columns_test.append(\"ID\")\n",
    "for i in range(38):\n",
    "    num = \"v\"+str(i+1)\n",
    "    columns_test.append(num)\n",
    "    \n",
    "df_test = pd.DataFrame(columns=columns_test)\n",
    "\n",
    "\n",
    "for x in test_list:\n",
    "    with open(x, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        new_row = []\n",
    "        scene_id = data['scene_idx']\n",
    "        new_row.append(scene_id)\n",
    "        agent_id = data['agent_id']\n",
    "        idx = np.where(data[\"track_id\"] == data[\"agent_id\"])[0][0]   \n",
    "        input_data = data['p_in'][idx,:,:]\n",
    "        for i in range(38):\n",
    "            new_row.append(input_data.flatten()[i])\n",
    "        df_length = len(df_test)\n",
    "        df_test.loc[df_length] = new_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'./lstm_y.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
